{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding (BPE) Tokenization Tutorial\n",
    "\n",
    "Welcome to this comprehensive tutorial on Byte Pair Encoding (BPE), one of the most important tokenization algorithms in modern Natural Language Processing!\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. What tokenization is and why it matters\n",
    "2. The problem BPE solves\n",
    "3. How the BPE algorithm works step-by-step\n",
    "4. Implementing BPE from scratch in Python\n",
    "5. How BPE is used in modern language models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Tokenization\n",
    "\n",
    "**Tokenization** is the process of breaking text into smaller units called \"tokens\". These tokens are the fundamental units that language models process.\n",
    "\n",
    "### Why Do We Need Tokenization?\n",
    "\n",
    "Language models work with numbers, not text. Tokenization converts text into numerical representations that models can understand.\n",
    "\n",
    "### Common Tokenization Approaches:\n",
    "\n",
    "1. **Character-level**: Each character is a token\n",
    "   - Pros: Small vocabulary, handles any text\n",
    "   - Cons: Very long sequences, loses word meaning\n",
    "\n",
    "2. **Word-level**: Each word is a token\n",
    "   - Pros: Preserves word meaning\n",
    "   - Cons: Huge vocabulary, can't handle unknown words\n",
    "\n",
    "3. **Subword-level**: Words are broken into meaningful parts (BPE falls here!)\n",
    "   - Pros: Balance between vocabulary size and sequence length\n",
    "   - Cons: More complex implementation\n",
    "\n",
    "**BPE is used in GPT, BERT, RoBERTa, and many other modern language models!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Problem BPE Solves\n",
    "\n",
    "Let's see the problems with simple approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character tokens (62 tokens):\n",
      "['T', 'h', 'e', ' ', 'q', 'u', 'i', 'c', 'k', ' ', 'b', 'r', 'o', 'w', 'n', ' ', 'f', 'o', 'x', ' ', 'j', 'u', 'm', 'p', 's', ' ', 'o', 'v', 'e', 'r'] ...\n",
      "\n",
      "Word tokens (13 tokens):\n",
      "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.', 'the', 'fox', 'is', 'quick!']\n",
      "\n",
      "Unique characters: 30\n",
      "Unique words: 10\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "text = \"The quick brown fox jumps over the lazy dog. The fox is quick!\"\n",
    "\n",
    "# Character-level tokenization\n",
    "char_tokens = list(text)\n",
    "print(f\"Character tokens ({len(char_tokens)} tokens):\")\n",
    "print(char_tokens[:30], \"...\\n\")\n",
    "\n",
    "# Word-level tokenization\n",
    "word_tokens = text.lower().split()\n",
    "print(f\"Word tokens ({len(word_tokens)} tokens):\")\n",
    "print(word_tokens)\n",
    "print(f\"\\nUnique characters: {len(set(char_tokens))}\")\n",
    "print(f\"Unique words: {len(set(word_tokens))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems:\n",
    "\n",
    "- **Character-level**: 58 tokens for a short sentence! Sequences get very long.\n",
    "- **Word-level**: What about \"jumping\" vs \"jumps\"? They're treated as completely different words, even though they share meaning.\n",
    "- **Unknown words**: Word-level can't handle words not in the vocabulary.\n",
    "\n",
    "**BPE Solution**: Learn frequent subword units automatically from the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. How BPE Works: The Algorithm\n",
    "\n",
    "BPE is beautifully simple! It works in two phases:\n",
    "\n",
    "### Training Phase:\n",
    "\n",
    "1. Start with a vocabulary of all individual characters\n",
    "2. Split all words into sequences of characters\n",
    "3. Repeatedly:\n",
    "   - Find the most frequent pair of adjacent tokens\n",
    "   - Merge this pair into a new token\n",
    "   - Add the new token to the vocabulary\n",
    "4. Stop after a fixed number of merges (hyperparameter)\n",
    "\n",
    "### Encoding Phase:\n",
    "\n",
    "1. Start with word split into characters\n",
    "2. Apply learned merge rules in order\n",
    "3. Result: word encoded as sequence of subword tokens\n",
    "\n",
    "### Visual Example:\n",
    "\n",
    "```\n",
    "Corpus: \"low\" (5 times), \"lower\" (2 times), \"newest\" (6 times), \"widest\" (3 times)\n",
    "\n",
    "Initial tokens (characters): l o w e r n w i d s t\n",
    "\n",
    "Step 1: Most frequent pair is 'e' + 's' → merge into 'es'\n",
    "  \"newest\" → n e w e s t → n e w es t\n",
    "  \"widest\" → w i d e s t → w i d es t\n",
    "\n",
    "Step 2: Most frequent pair is 'es' + 't' → merge into 'est'\n",
    "  \"newest\" → n e w est\n",
    "  \"widest\" → w i d est\n",
    "\n",
    "Step 3: Most frequent pair is 'l' + 'o' → merge into 'lo'\n",
    "  \"low\" → lo w\n",
    "  \"lower\" → lo w e r\n",
    "\n",
    "... and so on!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementing BPE from Scratch\n",
    "\n",
    "Let's build our own BPE tokenizer step by step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE implementation ready!\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "\n",
    "class BytePairEncoding:\n",
    "    def __init__(self, num_merges=10):\n",
    "        \"\"\"\n",
    "        Initialize BPE tokenizer.\n",
    "        \n",
    "        Args:\n",
    "            num_merges: Number of merge operations to perform\n",
    "        \"\"\"\n",
    "        self.num_merges = num_merges\n",
    "        self.merges = {}  # Store merge rules: (pair) -> merged_token\n",
    "        self.vocab = {}   # Final vocabulary\n",
    "        \n",
    "    def get_pairs(self, word):\n",
    "        \"\"\"\n",
    "        Get all adjacent pairs of tokens in a word.\n",
    "        \n",
    "        Args:\n",
    "            word: List of tokens\n",
    "            \n",
    "        Returns:\n",
    "            Set of adjacent pairs\n",
    "        \"\"\"\n",
    "        pairs = set()\n",
    "        prev_token = word[0]\n",
    "        for token in word[1:]:\n",
    "            pairs.add((prev_token, token))\n",
    "            prev_token = token\n",
    "        return pairs\n",
    "    \n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Train BPE on a corpus of text.\n",
    "        \n",
    "        Args:\n",
    "            corpus: List of words or string of text\n",
    "        \"\"\"\n",
    "        # Convert to list of words if string\n",
    "        if isinstance(corpus, str):\n",
    "            corpus = corpus.lower().split()\n",
    "        \n",
    "        # Count word frequencies\n",
    "        word_freqs = Counter(corpus)\n",
    "        \n",
    "        # Initialize vocabulary with character-level tokens\n",
    "        # Add </w> to mark end of word\n",
    "        vocab = {}\n",
    "        for word, freq in word_freqs.items():\n",
    "            # Split into characters and add end-of-word marker\n",
    "            tokens = list(word) + ['</w>']\n",
    "            vocab[' '.join(tokens)] = freq\n",
    "        \n",
    "        print(f\"Initial vocabulary size: {len(set(token for word in vocab.keys() for token in word.split()))}\")\n",
    "        print(f\"Example initial splits: {list(vocab.keys())[:3]}\\n\")\n",
    "        \n",
    "        # Perform merges\n",
    "        for i in range(self.num_merges):\n",
    "            # Count all pairs\n",
    "            pairs = defaultdict(int)\n",
    "            for word, freq in vocab.items():\n",
    "                symbols = word.split()\n",
    "                for pair in self.get_pairs(symbols):\n",
    "                    pairs[pair] += freq\n",
    "            \n",
    "            if not pairs:\n",
    "                break\n",
    "            \n",
    "            # Find most frequent pair\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            print(f\"Merge {i+1}: ('{best_pair[0]}', '{best_pair[1]}') → '{best_pair[0]+best_pair[1]}' (frequency: {pairs[best_pair]})\")\n",
    "            \n",
    "            # Store merge rule\n",
    "            self.merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "            \n",
    "            # Apply merge to vocabulary\n",
    "            new_vocab = {}\n",
    "            bigram = ' '.join(best_pair)\n",
    "            replacement = best_pair[0] + best_pair[1]\n",
    "            \n",
    "            for word, freq in vocab.items():\n",
    "                new_word = word.replace(bigram, replacement)\n",
    "                new_vocab[new_word] = freq\n",
    "            \n",
    "            vocab = new_vocab\n",
    "        \n",
    "        # Build final vocabulary\n",
    "        self.vocab = set()\n",
    "        for word in vocab.keys():\n",
    "            self.vocab.update(word.split())\n",
    "        \n",
    "        print(f\"\\nFinal vocabulary size: {len(self.vocab)}\")\n",
    "        print(f\"Sample tokens: {sorted(list(self.vocab))[:20]}\")\n",
    "    \n",
    "    def encode(self, word):\n",
    "        \"\"\"\n",
    "        Encode a word using learned BPE merges.\n",
    "        \n",
    "        Args:\n",
    "            word: String to encode\n",
    "            \n",
    "        Returns:\n",
    "            List of tokens\n",
    "        \"\"\"\n",
    "        word = word.lower()\n",
    "        # Start with characters\n",
    "        tokens = list(word) + ['</w>']\n",
    "        \n",
    "        # Apply merges in order\n",
    "        while len(tokens) > 1:\n",
    "            pairs = self.get_pairs(tokens)\n",
    "            if not pairs:\n",
    "                break\n",
    "            \n",
    "            # Find first applicable merge rule\n",
    "            # (We need to apply merges in the order they were learned)\n",
    "            bigram = None\n",
    "            for merge in self.merges:\n",
    "                if merge in pairs:\n",
    "                    bigram = merge\n",
    "                    break\n",
    "            \n",
    "            if not bigram:\n",
    "                break\n",
    "            \n",
    "            # Apply the merge\n",
    "            first, second = bigram\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                if i < len(tokens) - 1 and tokens[i] == first and tokens[i+1] == second:\n",
    "                    new_tokens.append(first + second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "            tokens = new_tokens\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "print(\"BPE implementation ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training BPE on a Sample Corpus\n",
    "\n",
    "Let's train our BPE tokenizer on a small corpus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial vocabulary size: 13\n",
      "Example initial splits: ['l o w </w>', 'l o w e r </w>', 'l o w e s t </w>']\n",
      "\n",
      "Merge 1: ('w', 'e') → 'we' (frequency: 15)\n",
      "Merge 2: ('t', '</w>') → 't</w>' (frequency: 13)\n",
      "Merge 3: ('s', 't</w>') → 'st</w>' (frequency: 13)\n",
      "Merge 4: ('r', '</w>') → 'r</w>' (frequency: 11)\n",
      "Merge 5: ('l', 'o') → 'lo' (frequency: 10)\n",
      "Merge 6: ('n', 'e') → 'ne' (frequency: 10)\n",
      "Merge 7: ('ne', 'we') → 'newe' (frequency: 10)\n",
      "Merge 8: ('newe', 'st</w>') → 'newest</w>' (frequency: 6)\n",
      "Merge 9: ('d', 'e') → 'de' (frequency: 6)\n",
      "Merge 10: ('w', 'i') → 'wi' (frequency: 6)\n",
      "Merge 11: ('wi', 'de') → 'wide' (frequency: 6)\n",
      "Merge 12: ('lo', 'w') → 'low' (frequency: 5)\n",
      "Merge 13: ('low', '</w>') → 'low</w>' (frequency: 5)\n",
      "Merge 14: ('newe', 'r</w>') → 'newer</w>' (frequency: 4)\n",
      "Merge 15: ('lowe', 'st</w>') → 'lowest</w>' (frequency: 3)\n",
      "\n",
      "Final vocabulary size: 12\n",
      "Sample tokens: ['e', 'g', 'h', 'i', 'low</w>', 'lowe', 'lowest</w>', 'newer</w>', 'newest</w>', 'r</w>', 'st</w>', 'wide']\n"
     ]
    }
   ],
   "source": [
    "# Sample corpus\n",
    "corpus = \"\"\"\n",
    "low low low low low\n",
    "lower lower\n",
    "lowest lowest lowest\n",
    "newer newer newer newer\n",
    "newest newest newest newest newest newest\n",
    "wider wider wider\n",
    "widest widest widest\n",
    "higher higher\n",
    "highest\n",
    "\"\"\"\n",
    "\n",
    "# Initialize and train BPE\n",
    "bpe = BytePairEncoding(num_merges=15)\n",
    "bpe.train(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Merges\n",
    "\n",
    "Notice how BPE learned meaningful subword units:\n",
    "- It merged common endings like \"est\"\n",
    "- It learned common prefixes and roots\n",
    "- Frequent words get merged into single tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Encoding Words with BPE\n",
    "\n",
    "Now let's use our trained BPE to encode some words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Encoding Results:\n",
      "==================================================\n",
      "'lowest' → ['lo', 'we', 'st</w>']\n",
      "  (6 chars → 3 tokens)\n",
      "\n",
      "'newer' → ['newer</w>']\n",
      "  (5 chars → 1 tokens)\n",
      "\n",
      "'highest' → ['h', 'i', 'g', 'h', 'e', 'st</w>']\n",
      "  (7 chars → 6 tokens)\n",
      "\n",
      "'lower' → ['lo', 'we', 'r</w>']\n",
      "  (5 chars → 3 tokens)\n",
      "\n",
      "'newer' → ['newer</w>']\n",
      "  (5 chars → 1 tokens)\n",
      "\n",
      "'fastest' → ['f', 'a', 's', 't', 'e', 'st</w>']\n",
      "  (7 chars → 6 tokens)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test encoding\n",
    "test_words = ['lowest', 'newer', 'highest', 'lower', 'newer', 'fastest']\n",
    "\n",
    "print(\"BPE Encoding Results:\")\n",
    "print(\"=\" * 50)\n",
    "for word in test_words:\n",
    "    tokens = bpe.encode(word)\n",
    "    print(f\"'{word}' → {tokens}\")\n",
    "    print(f\"  ({len(word)} chars → {len(tokens)} tokens)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "- **Seen words**: Words in the training corpus are encoded efficiently\n",
    "- **Unseen words**: Even words not in the training corpus (like 'fastest') can be encoded!\n",
    "- **Shared subwords**: Similar words share common subword tokens (like 'low' in 'lower' and 'lowest')\n",
    "- **Graceful degradation**: In the worst case, we fall back to character-level encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing BPE in Action\n",
    "\n",
    "Let's create a visual comparison of different tokenization methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization Comparison\n",
      "======================================================================\n",
      "Original text: 'the newest lowest highest'\n",
      "\n",
      "Character-level (25 tokens):\n",
      "  ['t', 'h', 'e', ' ', 'n', 'e', 'w', 'e', 's', 't', ' ', 'l', 'o', 'w', 'e', 's', 't', ' ', 'h', 'i', 'g', 'h', 'e', 's', 't']\n",
      "\n",
      "Word-level (4 tokens):\n",
      "  ['the', 'newest', 'lowest', 'highest']\n",
      "\n",
      "BPE (14 tokens):\n",
      "  ['t', 'h', 'e', '</w>', 'newest</w>', 'lo', 'we', 'st</w>', 'h', 'i', 'g', 'h', 'e', 'st</w>']\n",
      "\n",
      "Summary:\n",
      "  Character tokens: 25\n",
      "  Word tokens: 4\n",
      "  BPE tokens: 14 ← Best balance!\n"
     ]
    }
   ],
   "source": [
    "def compare_tokenizations(text, bpe_tokenizer):\n",
    "    \"\"\"\n",
    "    Compare character, word, and BPE tokenization.\n",
    "    \"\"\"\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    # Character-level\n",
    "    char_tokens = list(text.lower())\n",
    "    \n",
    "    # Word-level\n",
    "    word_tokens = words\n",
    "    \n",
    "    # BPE\n",
    "    bpe_tokens = []\n",
    "    for word in words:\n",
    "        bpe_tokens.extend(bpe_tokenizer.encode(word))\n",
    "    \n",
    "    print(\"Tokenization Comparison\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Original text: '{text}'\\n\")\n",
    "    \n",
    "    print(f\"Character-level ({len(char_tokens)} tokens):\")\n",
    "    print(f\"  {char_tokens}\\n\")\n",
    "    \n",
    "    print(f\"Word-level ({len(word_tokens)} tokens):\")\n",
    "    print(f\"  {word_tokens}\\n\")\n",
    "    \n",
    "    print(f\"BPE ({len(bpe_tokens)} tokens):\")\n",
    "    print(f\"  {bpe_tokens}\\n\")\n",
    "    \n",
    "    print(\"Summary:\")\n",
    "    print(f\"  Character tokens: {len(char_tokens)}\")\n",
    "    print(f\"  Word tokens: {len(word_tokens)}\")\n",
    "    print(f\"  BPE tokens: {len(bpe_tokens)} ← Best balance!\")\n",
    "\n",
    "# Test it\n",
    "test_text = \"the newest lowest highest\"\n",
    "compare_tokenizations(test_text, bpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Advantages of BPE\n",
    "\n",
    "### 1. **Vocabulary Efficiency**\n",
    "- Controlled vocabulary size (you choose num_merges)\n",
    "- Much smaller than word-level\n",
    "- Much more meaningful than character-level\n",
    "\n",
    "### 2. **Handles Unknown Words**\n",
    "- Can tokenize any word, even if not in training data\n",
    "- Falls back to characters in worst case\n",
    "- No \"unknown token\" needed!\n",
    "\n",
    "### 3. **Captures Morphology**\n",
    "- Learns prefixes (un-, re-, pre-)\n",
    "- Learns suffixes (-ing, -ed, -est)\n",
    "- Learns common roots\n",
    "\n",
    "### 4. **Data-Driven**\n",
    "- No linguistic knowledge required\n",
    "- Adapts to any language\n",
    "- Learns from actual text distribution\n",
    "\n",
    "### 5. **Compression**\n",
    "- Reduces sequence length compared to characters\n",
    "- Maintains meaning better than characters\n",
    "- Faster training and inference for models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. BPE in Modern Language Models\n",
    "\n",
    "BPE is used in many famous models:\n",
    "\n",
    "- **GPT-2/GPT-3**: Uses BPE with ~50,000 merges\n",
    "- **RoBERTa**: Uses byte-level BPE\n",
    "- **BART**: Uses BPE tokenization\n",
    "- **T5**: Uses SentencePiece (BPE variant)\n",
    "\n",
    "### Real-world Considerations:\n",
    "\n",
    "1. **Byte-level BPE**: Instead of characters, start with bytes (0-255)\n",
    "   - Handles any Unicode character\n",
    "   - Used in GPT-2\n",
    "\n",
    "2. **Pre-tokenization**: Often split on whitespace and punctuation first\n",
    "\n",
    "3. **Special tokens**: Add special tokens like `<|endoftext|>`, `<PAD>`, etc.\n",
    "\n",
    "4. **Vocabulary size**: Typically 30,000-50,000 tokens\n",
    "   - Smaller = longer sequences, less memory\n",
    "   - Larger = shorter sequences, more parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Hands-on Exercise\n",
    "\n",
    "Try training BPE on your own text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial vocabulary size: 23\n",
      "Example initial splits: ['m a c h i n e </w>', 'l e a r n i n g </w>', 'i s </w>']\n",
      "\n",
      "Merge 1: ('i', 'n') → 'in' (frequency: 8)\n",
      "Merge 2: ('a', 'r') → 'ar' (frequency: 8)\n",
      "Merge 3: ('g', '</w>') → 'g</w>' (frequency: 7)\n",
      "Merge 4: ('in', 'g</w>') → 'ing</w>' (frequency: 7)\n",
      "Merge 5: ('s', '</w>') → 's</w>' (frequency: 7)\n",
      "Merge 6: ('e', 'ar') → 'ear' (frequency: 6)\n",
      "Merge 7: ('ear', 'n') → 'earn' (frequency: 6)\n",
      "Merge 8: ('l', 'earn') → 'learn' (frequency: 6)\n",
      "Merge 9: ('learn', 'ing</w>') → 'learning</w>' (frequency: 5)\n",
      "Merge 10: ('e', 'r') → 'er' (frequency: 4)\n",
      "\n",
      "Final vocabulary size: 30\n",
      "Sample tokens: ['</w>', 'a', 'ar', 'c', 'd', 'e', 'er', 'f', 'g', 'h', 'i', 'in', 'ing</w>', 'k', 'l', 'learn', 'learning</w>', 'm', 'n', 'o']\n",
      "\n",
      "Test encoding:\n",
      "'learning' → ['learning</w>']\n",
      "'machine' → ['m', 'a', 'c', 'h', 'in', 'e', '</w>']\n",
      "'neural' → ['n', 'e', 'u', 'r', 'a', 'l', '</w>']\n",
      "'supervised' → ['s', 'u', 'p', 'er', 'v', 'i', 's', 'e', 'd', '</w>']\n"
     ]
    }
   ],
   "source": [
    "# Exercise: Train BPE on a custom corpus\n",
    "\n",
    "# TODO: Replace this with your own text!\n",
    "my_corpus = \"\"\"\n",
    "machine learning is amazing\n",
    "deep learning is powerful\n",
    "learning algorithms are interesting\n",
    "neural networks are learning systems\n",
    "unsupervised learning learns patterns\n",
    "\"\"\"\n",
    "\n",
    "# Train a BPE tokenizer\n",
    "my_bpe = BytePairEncoding(num_merges=10)\n",
    "my_bpe.train(my_corpus)\n",
    "\n",
    "# Test it\n",
    "print(\"\\nTest encoding:\")\n",
    "test_words = ['learning', 'machine', 'neural', 'supervised']\n",
    "for word in test_words:\n",
    "    print(f\"'{word}' → {my_bpe.encode(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise Questions:\n",
    "\n",
    "1. What subword units did BPE learn from your corpus?\n",
    "2. Try encoding a word that wasn't in your training corpus. How does BPE handle it?\n",
    "3. Experiment with different values of `num_merges`. How does it affect the results?\n",
    "4. Add more examples of related words (e.g., \"learn\", \"learned\", \"learner\"). Does BPE capture the relationship?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Advanced Topics\n",
    "\n",
    "### BPE Variants:\n",
    "\n",
    "1. **Byte-Level BPE**:\n",
    "   - Operates on bytes instead of characters\n",
    "   - Vocabulary of 256 base tokens (all possible bytes)\n",
    "   - Can handle any Unicode text\n",
    "\n",
    "2. **WordPiece**:\n",
    "   - Used in BERT\n",
    "   - Instead of frequency, uses likelihood to choose merges\n",
    "   - Adds special prefix (##) for continuing subwords\n",
    "\n",
    "3. **SentencePiece**:\n",
    "   - Treats text as Unicode characters\n",
    "   - No pre-tokenization needed\n",
    "   - Language-agnostic\n",
    "\n",
    "### Implementation Tips:\n",
    "\n",
    "- **Caching**: Store merge results to speed up encoding\n",
    "- **Parallel processing**: Train on multiple documents simultaneously\n",
    "- **Incremental training**: Update vocabulary with new data\n",
    "- **Vocabulary pruning**: Remove rare tokens to save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Using Production BPE Libraries\n",
    "\n",
    "For real projects, use optimized libraries like `tokenizers` from Hugging Face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokens: ['machine', 'learning', 'is', 'fascinating']\n",
      "IDs: [93, 92, 55, 86]\n"
     ]
    }
   ],
   "source": [
    "# Note: You'll need to install the tokenizers library first\n",
    "# pip install tokenizers\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Configure the trainer\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"], vocab_size=1000)\n",
    "\n",
    "# Train on files\n",
    "# tokenizer.train(files=[\"corpus.txt\"], trainer=trainer)\n",
    "\n",
    "# Or train on an iterator\n",
    "corpus_list = [\n",
    "    \"the quick brown fox jumps over the lazy dog\",\n",
    "    \"machine learning is fascinating\",\n",
    "    \"natural language processing with transformers\"\n",
    "]\n",
    "tokenizer.train_from_iterator(corpus_list, trainer=trainer)\n",
    "\n",
    "# Encode text\n",
    "output = tokenizer.encode(\"machine learning is fascinating\")\n",
    "print(f\"Tokens: {output.tokens}\")\n",
    "print(f\"IDs: {output.ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary and Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Tokenization** converts text into processable units\n",
    "2. **BPE** finds the sweet spot between character and word-level tokenization\n",
    "3. The algorithm is **simple**: iteratively merge the most frequent pairs\n",
    "4. BPE is **data-driven** and learns meaningful subword units\n",
    "5. It **handles unknown words** gracefully\n",
    "6. BPE is used in **most modern language models**\n",
    "\n",
    "### When to Use BPE:\n",
    "\n",
    "✅ Building language models  \n",
    "✅ Working with morphologically rich languages  \n",
    "✅ Need to handle unknown words  \n",
    "✅ Want to control vocabulary size  \n",
    "✅ Working with limited training data  \n",
    "\n",
    "### Further Reading:\n",
    "\n",
    "- Original BPE paper: \"Neural Machine Translation of Rare Words with Subword Units\" (Sennrich et al., 2016)\n",
    "- GPT-2 paper for byte-level BPE: \"Language Models are Unsupervised Multitask Learners\"\n",
    "- Hugging Face tokenizers documentation: https://huggingface.co/docs/tokenizers/\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations! 🎉\n",
    "\n",
    "You now understand how BPE tokenization works and how it powers modern language models. Try experimenting with different corpora and parameters to build intuition!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
